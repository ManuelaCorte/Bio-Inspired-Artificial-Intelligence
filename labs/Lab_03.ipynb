{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "# Goal.\n",
    "The goal of this lab is to familiarize yourself with some advanced forms of evolutionary computation (EC). In particular, you will explore the use of Evolution Strategies (ES) and Covariance Matrix Adaptation Evolution Strategies (CMA-ES). You will observe the effects of different forms of self-adaptation and how these are useful for black-box optimization.\n",
    "\n",
    "Note once again that, unless otherwise specified, in this module's exercises we will use real-valued genotypes and that the aim of the algorithms will be to *minimize* the fitness function $f(\\mathbf{x})$, i.e. lower values correspond to a better fitness!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "In this exercise you will explore the use of Evolution Strategies (ES). ES are a popular class of evolutionary algorithms used for optimization problems. In particular you will explore the popular ($\\mu$/$\\rho$,$\\lambda$)-ES where $\\mu$ denotes the number of parents, $\\rho\\leq\\mu$ the number of parents involved in the producing a single offspring (mixing number), $\\lambda$ the number of offspring, and *comma* selection is employed.\n",
    "\n",
    "To start the experiments, run the next cell. Note that, as usual, you may change the seed forthe pseudo-random number generator. \n",
    "\n",
    "This code will attempt to optimize the 10-dimensional __[Rosenbrock function](http://pythonhosted.org/inspyred/reference.html\\#inspyred.benchmarks.Rosenbrock)__ using an Evolutionary Strategy without self-adaptation or recombination (which is very similar to the GA you used in the first module's exercises).\n",
    "\n",
    "Try adjusting the various parameters: $\\mu$, $\\lambda$, and $\\rho$, (by changing, respectively, `args[pop_size]`, `args[num_offspring]`, and `args[mixing_number]` in the next cell. \n",
    "- What happens if you make $\\lambda$ smaller e.g. $\\lambda=\\mu$? You select all offsprings instead of best $\\mu$ individuals\n",
    "- What happens if you increase the mixing number $\\rho$? Too little parents and we aren't exploiting the genetic material of the parents enough, too many parents and we lose diversity in the population\n",
    "- Does this confirm or contradict the conclusions you drew in the first module?\n",
    "\n",
    "\n",
    "Try out the different strategy modes (change the parameter `args[strategy_mode]`), and observe how they affect the performance of the algorithm:\n",
    "\n",
    "- `None` means that there is no self-adaptation\n",
    "- `es.GLOBAL` means each genome encodes a global step-size (mutation standard deviation)\n",
    "- `es.INDIVIDUAL` means each genome encodes an independent step-size for each gene.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "from inspyred import benchmarks\n",
    "\n",
    "from utils.utils_03.es import *\n",
    "from utils.utils_03.inspyred_utils import NumpyRandomWrapper\n",
    "\n",
    "display = True  # Plot initial and final populations\n",
    "\n",
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "Edit this part to do the exercises\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "num_vars = 10\n",
    "\n",
    "# parameters for the ES\n",
    "args = {}\n",
    "args[\"max_generations\"] = 100\n",
    "args[\"sigma\"] = 1.0  # default standard deviation\n",
    "\n",
    "args[\"pop_size\"] = 20  # mu\n",
    "args[\"num_offspring\"] = 100  # lambda\n",
    "\n",
    "# args[\"strategy_mode\"] = None\n",
    "args[\"strategy_mode\"] = GLOBAL\n",
    "# args[\"strategy_mode\"] = INDIVIDUAL\n",
    "\n",
    "# args[\"mixing_number\"] = 1 #rho\n",
    "args[\"mixing_number\"] = 5\n",
    "# args[\"mixing_number\"] = 20\n",
    "\n",
    "# args[\"problem_class\"] = benchmarks.Sphere\n",
    "args[\"problem_class\"] = benchmarks.Rosenbrock\n",
    "args[\"problem_class\"] = benchmarks.Rastrigin\n",
    "\n",
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "args[\"fig_title\"] = \"ES\"\n",
    "\n",
    "seed = 0\n",
    "rng = NumpyRandomWrapper(seed)\n",
    "# Run the ES\n",
    "best_individual, best_fitness, final_pop = run_es(\n",
    "    rng, num_vars=num_vars, display=display, use_log_scale=True, **args\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"Best Individual:\", best_individual)\n",
    "print(\"Best Fitness:\", best_fitness)\n",
    "\n",
    "if display:\n",
    "    ioff()\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "from inspyred import benchmarks\n",
    "\n",
    "from utils.utils_03.es import *\n",
    "from utils.utils_03.inspyred_utils import NumpyRandomWrapper\n",
    "\n",
    "display = True  # Plot initial and final populations\n",
    "\n",
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "Edit this part to do the exercises\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "num_vars = 10\n",
    "\n",
    "# parameters for the ES\n",
    "args = {}\n",
    "args[\"max_generations\"] = 100\n",
    "args[\"sigma\"] = 1.0  # default standard deviation\n",
    "\n",
    "args[\"pop_size\"] = 20  # mu\n",
    "args[\"num_offspring\"] = 100  # lambda\n",
    "\n",
    "args[\"strategy_mode\"] = None\n",
    "# args[\"strategy_mode\"] = GLOBAL\n",
    "# args[\"strategy_mode\"] = INDIVIDUAL\n",
    "\n",
    "# args[\"mixing_number\"] = 1 #rho\n",
    "args[\"mixing_number\"] = 5\n",
    "# args[\"mixing_number\"] = 20\n",
    "\n",
    "args[\"problem_class\"] = benchmarks.Sphere\n",
    "# args[\"problem_class\"] = benchmarks.Rosenbrock\n",
    "# args[\"problem_class\"] = benchmarks.Rastrigin\n",
    "\n",
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "args[\"fig_title\"] = \"ES\"\n",
    "\n",
    "seed = 0\n",
    "rng = NumpyRandomWrapper(seed)\n",
    "# Run the ES\n",
    "best_individual, best_fitness, final_pop = run_es(\n",
    "    rng, num_vars=num_vars, display=display, use_log_scale=True, **args\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"Best Individual:\", best_individual)\n",
    "print(\"Best Fitness:\", best_fitness)\n",
    "\n",
    "if display:\n",
    "    ioff()\n",
    "    show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "In this exercise you will systematically explore some of the intuitions you gained in Exercise 1. By default this code is configured to run $10$ ES runs apiece with each different strategy mode. The best fitnesses of each run are shown in a boxplot similar to what you saw in the first module's exercises.\n",
    "\n",
    "- How does the self-adaptation strategy influence performance on this problem?\n",
    "- Does what you see here confirm what you suspected from the previous exercise?\n",
    "\n",
    "\n",
    "Use the provided code as a template to systematically explore the other parameters.\n",
    "\n",
    "**Note**: To study the effect of parameters, it's important to change only **one parameter at a time**, i.e. keep everything fixed except for the parameter you want to study, and for each parameter under study create box plots to compare different values.\n",
    "\n",
    "**Note**: In order to make a fair comparison you must keep the number of function evaluations (i.e., $\\lambda$ $\\times$ `max_generations`) fixed $^{[1]}$ ).\n",
    "\n",
    "- How do the values of $\\mu$, $\\rho$, and $\\lambda$ influence the performance given a particular self-adaptation strategy and other parameters?\n",
    "- Can you come up with any rules of thumb for choosing these parameters?\n",
    "\n",
    "\n",
    "In order to see how general your results are you can explore different benchmark problems $^{[2]}$ on different numbers of variables. You can change the problem by changing the parameter `args[problem_class]` and the problem dimension (number of variables) by changing the variable `num_vars` in the next cell. Note that most benchmark problems are *scalable*, i.e. they can be defined for any number of variables.\n",
    "\n",
    "- Can you find a choice of parameters that works properly across several problems?\n",
    "\n",
    "\n",
    "---\n",
    "[1]:\n",
    "When you compare multiple algorithms on the same problem, the computational budget allotted to any of them should be same, in order to get a fair comparison. So, if you run experiments with different values of $\\lambda$, you should adjust `max_generations` accordingly, so that the total number of evaluations is consistent across experiments. E.g.: {$\\lambda=50$, `max_generations`$=100$}, {$\\lambda=100$, `max_generations`$=50$}, etc. One rule of thumb sometimes used in evolutionary computation papers and black-box optimization competitions is to run each algorithm for $5000 \\times n$ fitness evaluations, where $n$ is the problem dimension. Also bear in mind that some algorithms are not generational at all: in those cases, the stop criterion must be expressed in terms of absolute number of evaluations, or as a convergence condition.\n",
    "\n",
    "[2]:\n",
    "See __[link](https://pythonhosted.org/inspyred/reference.html\\#single-objective-benchmarks)__ for a list of single-objective benchmark problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "from inspyred import benchmarks\n",
    "\n",
    "from utils.utils_03.es import *\n",
    "from utils.utils_03.inspyred_utils import NumpyRandomWrapper\n",
    "\n",
    "display = False  # Plot initial and final populations\n",
    "\n",
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "Edit this part to do the exercises\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "num_vars = 10\n",
    "\n",
    "# parameters for the ES\n",
    "args = {}\n",
    "args[\"max_generations\"] = 100\n",
    "args[\"sigma\"] = 1.0  # default standard deviation\n",
    "\n",
    "args[\"pop_size\"] = 20  # mu\n",
    "args[\"num_offspring\"] = 100  # lambda\n",
    "args[\"mixing_number\"] = 5  # rho\n",
    "\n",
    "strategy_modes = [None, GLOBAL, INDIVIDUAL]\n",
    "\n",
    "# args[\"mixing_number\"] = 1 #rho\n",
    "\n",
    "# args[\"problem_class\"] = benchmarks.Sphere\n",
    "# args[\"problem_class\"] = benchmarks.Rosenbrock\n",
    "args[\"problem_class\"] = benchmarks.Rastrigin\n",
    "\n",
    "num_runs = 10  # Number of runs to be done for each condition\n",
    "\n",
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "args[\"fig_title\"] = \"ES\"\n",
    "\n",
    "seed = 0\n",
    "\n",
    "rng = NumpyRandomWrapper(seed)\n",
    "\n",
    "\n",
    "# Run the ES *num_runs* times for each strategy mode and record results\n",
    "results = []\n",
    "for strategy_mode in strategy_modes:\n",
    "    print(\"Trying strategy \", str(strategy_mode))\n",
    "    args[\"strategy_mode\"] = strategy_mode\n",
    "    results.append(\n",
    "        [\n",
    "            run_es(rng, num_vars=num_vars, display=display, **args)\n",
    "            for _ in range(num_runs)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "best_fitnesses = [[run_result[1] for run_result in runs] for runs in results]\n",
    "\n",
    "for fitness in best_fitnesses:\n",
    "    print(f\"min fitness: {min(fitness)}\")\n",
    "# Boxplot comparing the best fitnesses\n",
    "fig = figure(\"ES (best fitness)\")\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.boxplot(best_fitnesses)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xticklabels(strategy_modes)\n",
    "# ax.set_xticklabels( map(lambda s: str(s), strategy_modes ) )\n",
    "ax.set_xlabel(\"Strategy\")\n",
    "ax.set_ylabel(\"Fitness\")\n",
    "ioff()\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "In this exercise you will explore the use of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES). As described in the lecture CMA-ES is currently one of the most successful evolutionary optimizers. By using statistics gathered over generations it is able to adapt a covariance matrix in a completely derandomized way (in contrast to the ES we were just exploring).\n",
    "\n",
    "While *inspyred* does not include a native implementation of CMA-ES, we have made one available to you through a similar interface as you have been using so far. An example of proper usage is provided in next cell.\n",
    "\n",
    "Using this and the previous exercise as templates compare the performance of CMA-ES to the other Evolution Strategies you were just investigating.\n",
    "\n",
    "- Can CMA-ES find optima to different problems with fewer function evaluations?\n",
    "- How do these differences change with different pop. sizes and problem dimensions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "from inspyred import benchmarks\n",
    "\n",
    "from utils.utils_03.cma_es import *\n",
    "from utils.utils_03.es import *\n",
    "from utils.utils_03.inspyred_utils import NumpyRandomWrapper\n",
    "\n",
    "display = True  # Plot initial and final populations\n",
    "\n",
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "Edit this part to do the exercises\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "num_vars = 10\n",
    "\n",
    "# parameters for CMA-ES\n",
    "args = {}\n",
    "args[\"max_generations\"] = 200\n",
    "args[\"sigma\"] = 1.0  # default standard deviation\n",
    "\n",
    "\n",
    "args[\"pop_size\"] = 20  # mu\n",
    "args[\"num_offspring\"] = 100  # lambda\n",
    "\n",
    "# args[\"problem_class\"] = benchmarks.Sphere\n",
    "args[\"problem_class\"] = benchmarks.Rosenbrock\n",
    "# args[\"problem_class\"] = benchmarks.Rastrigin\n",
    "\n",
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "args[\"fig_title\"] = \"CMA-ES\"\n",
    "\n",
    "seed = 0\n",
    "rng = NumpyRandomWrapper(seed)\n",
    "# Run CMA-ES\n",
    "best_individual, best_fitness = run(\n",
    "    rng, num_vars=num_vars, display=display, use_log_scale=True, **args\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"Best Individual:\", best_individual)\n",
    "print(\"Best Fitness:\", best_fitness)\n",
    "\n",
    "if display:\n",
    "    ioff()\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Instructions and questions\n",
    "\n",
    "Concisely note down your observations from the previous exercises (follow the bullet points) and think about the following questions.\n",
    "\n",
    "- Do the observations you made while varying μ, ρ, and λ confirm or contradict the conclusions you drew in the previous module's exercises?\n",
    "- What are the advantages of self-adaptation in evolutionary computation?\n",
    "- In what ways might self-adaptation be occurring in biological organisms?\n",
    "- Compare the different self-adaptation strategies explored in this exercise. \n",
    "In what ways are certain strategies better than others for optimization? \n",
    "In what ways are certain strategies more biologically plausible than others?\n",
    "- Describe what reasons may contribute to better performance of CMA-ES and what can be the conditions \n",
    "when CMA-ES is not better than a basic ES."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
