{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Goal\n",
    "The goal of this lab is to familiarize yourself with the most important techniques for neuro-evolution. You will first explore the basic neuro-evolution techniques and then move on to more advanced algorithms such as Neural Evolution of Augmenting Topologies (NEAT).\n",
    "\n",
    "This lab continues the use of the *inspyred* framework for the Python programming language seen in the previous labs. If you did not participate in the previous labs, you may want to look that over first and then start this lab's exercises. Furthermore, in this lab we will use another Python library, *neat-python*, that contains a complete implementation of **NEAT** ([link here](https://neat-python.readthedocs.io)).\n",
    "\n",
    "\n",
    "Note that, unless otherwise specified, in this module's exercises we will use real-valued genotypes and that the aim of the algorithms will be to *minimize* the training error function of the evolved Neural Networks, i.e., lower values correspond to a better fitness!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "\n",
    "class NeuralNet:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_inputs: int,\n",
    "        num_outputs: int,\n",
    "        num_hidden: int = 0,\n",
    "        recurrent: bool = False,\n",
    "        batch_size: int = 1,\n",
    "    ):\n",
    "        self.weights: list[NDArray[np.float64]] = []\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.num_hidden = num_hidden\n",
    "        self.recurrent = recurrent\n",
    "\n",
    "        # if there is a hidden layer, we feed all inputs to hidden and all\n",
    "        # hidden to outputs\n",
    "\n",
    "        # if recurrent, we feedback hidden activations to hidden\n",
    "        # layer (or output to output layer, if there is no hidden layer)\n",
    "\n",
    "        # here we create data structures for weights\n",
    "        # NOTE: we consider the bias to be an extra input to each layer for\n",
    "        # ease of calculation\n",
    "        if num_hidden == 0:\n",
    "            n = num_inputs + 1\n",
    "            if recurrent:\n",
    "                n += num_outputs\n",
    "            m = num_outputs\n",
    "            self.weights.append(np.zeros((n, m), dtype=float))\n",
    "\n",
    "        else:\n",
    "            # input to hidden matrix first\n",
    "            n1 = num_inputs + 1\n",
    "            if recurrent:\n",
    "                n1 += num_hidden\n",
    "            m1 = num_hidden\n",
    "            self.weights.append(np.zeros((n1, m1), dtype=float))\n",
    "\n",
    "            # and now hidden to output\n",
    "            n2 = num_hidden + 1\n",
    "            m2 = num_outputs\n",
    "\n",
    "            self.weights.append(np.zeros((n2, m2), dtype=float))\n",
    "\n",
    "        self.num_params = sum([w.size for w in self.weights])\n",
    "\n",
    "        # create vector for neuron activations\n",
    "        self.reset(batch_size)\n",
    "\n",
    "    def reset(self, batch_size: int = 1):\n",
    "        self.batch_size = batch_size\n",
    "        # zero out all activations\n",
    "        self.activations = np.zeros((batch_size, self.num_hidden + self.num_outputs))\n",
    "\n",
    "    def set_params(self, params: list[float] | NDArray[np.float64]) -> None:\n",
    "        if len(params) != self.num_params:\n",
    "            raise Exception(\n",
    "                \"Incorrect number of params! Expected \"\n",
    "                + str(self.num_params)\n",
    "                + \", but received \"\n",
    "                + str(len(params))\n",
    "            )\n",
    "\n",
    "        self.weights[0][:, :] = np.asarray(params[: self.weights[0].size]).reshape(\n",
    "            self.weights[0].shape\n",
    "        )\n",
    "        if self.num_hidden > 0:\n",
    "            self.weights[1][:, :] = np.asarray(params[self.weights[0].size :]).reshape(\n",
    "                self.weights[1].shape\n",
    "            )\n",
    "\n",
    "    def step(self, inputs: NDArray[np.int64]) -> NDArray[np.float64]:  # type: ignore\n",
    "        # step the network and return the current output activations\n",
    "        inputs: NDArray[np.float64] = np.asarray(inputs, dtype=float)\n",
    "        one_dimensional = len(inputs.shape) == 1\n",
    "\n",
    "        # first, if just given a 1D input array, convert to a matrix\n",
    "        if one_dimensional:\n",
    "            inputs = inputs[None, :]  # adds a second dimension\n",
    "        if inputs.shape[0] != self.batch_size:\n",
    "            raise Exception(\n",
    "                \"Incorrect batch size! Should be \"\n",
    "                + str(self.batch_size)\n",
    "                + \", but is \"\n",
    "                + str(inputs.shape[0])\n",
    "            )\n",
    "        if inputs.shape[1] != self.num_inputs:\n",
    "            raise Exception(\n",
    "                \"Incorrect number of inputs! Should be \"\n",
    "                + str(self.num_inputs)\n",
    "                + \", but is \"\n",
    "                + str(inputs.shape[1])\n",
    "            )\n",
    "\n",
    "        # second, add in biases and current hidden activations (if recurrent)\n",
    "        if self.recurrent:\n",
    "            if self.num_hidden > 0:\n",
    "                input_values = np.hstack((\n",
    "                    inputs,\n",
    "                    self.activations[:, : self.num_hidden],\n",
    "                    np.ones((self.batch_size, 1)),\n",
    "                ))\n",
    "            else:\n",
    "                input_values = np.hstack((\n",
    "                    inputs,\n",
    "                    self.activations[:, :],\n",
    "                    np.ones((self.batch_size, 1)),\n",
    "                ))\n",
    "        else:\n",
    "            input_values = np.hstack((inputs, np.ones((self.batch_size, 1))))\n",
    "\n",
    "        # now calculate new activations by taking dot product and applying sigmoid\n",
    "        new_activations = 1.0 / (\n",
    "            1 + np.exp(-1.0 * np.dot(input_values, self.weights[0]))\n",
    "        )\n",
    "        if self.num_hidden == 0:\n",
    "            self.activations[:, :] = new_activations\n",
    "        else:\n",
    "            # use old hidden activations to compute outputs\n",
    "            hiddens = np.hstack((\n",
    "                self.activations[:, : self.num_hidden],\n",
    "                np.ones((self.batch_size, 1)),\n",
    "            ))\n",
    "            self.activations[:, self.num_hidden :] = 1.0 / (\n",
    "                1 + np.exp(-1.0 * np.dot(hiddens, self.weights[1]))\n",
    "            )\n",
    "            self.activations[:, : self.num_hidden] = new_activations\n",
    "\n",
    "        # finally, convert back to 1D, if that's what we started with\n",
    "        # otherwise return the matrix\n",
    "        if one_dimensional:\n",
    "            return self.activations[0, self.num_hidden :]\n",
    "        else:\n",
    "            return self.activations[:, self.num_hidden :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspyred import ec\n",
    "from inspyred.benchmarks import Benchmark\n",
    "import numpy as np\n",
    "from typing import Any, Callable\n",
    "from numpy.typing import NDArray\n",
    "from utils.inspyred_utils import NumpyRandomWrapper\n",
    "from abc import abstractmethod\n",
    "\n",
    "MAX_WEIGHT: int = 8\n",
    "\n",
    "INPUTS: NDArray[np.int64] = np.zeros((4, 2), dtype=int)\n",
    "for i in range(4):\n",
    "    INPUTS[i, :] = np.array([1 if i < 2 else 0, 1 if i % 2 == 0 else 0])\n",
    "\n",
    "\n",
    "class NeuralNetworkBenchmark(Benchmark):\n",
    "    \"\"\"Defines the base class for Neural Network Benchmark Problems.  Other\n",
    "    neural net benchmarks should inherit from this\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, net: NeuralNet):\n",
    "        self.net = net\n",
    "        super(NeuralNetworkBenchmark, self).__init__(self.net.num_params)\n",
    "        self.bounder = ec.Bounder(\n",
    "            [-MAX_WEIGHT] * self.dimensions, [MAX_WEIGHT] * self.dimensions\n",
    "        )\n",
    "\n",
    "    def evaluator(\n",
    "        self, candidates: NDArray[np.float64], args: Any\n",
    "    ) -> NDArray[np.float64]:\n",
    "        return np.array(list(map(self.evaluate_single, candidates)))\n",
    "\n",
    "    def generator(\n",
    "        self, random: NumpyRandomWrapper, args: dict[str, Any]\n",
    "    ) -> NDArray[np.float64]:\n",
    "        return np.asarray([\n",
    "            random.uniform(-MAX_WEIGHT, MAX_WEIGHT) for _ in range(self.net.num_params)\n",
    "        ])\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate_single(self, candidate: list[float]) -> int:\n",
    "        pass\n",
    "\n",
    "\n",
    "class BaseLogicBenchmark(NeuralNetworkBenchmark):\n",
    "    \"\"\"Defines the base class for single time step logic\n",
    "    Neural Network Benchmark Problems. Other logic neural net benchmarks\n",
    "    should inherit from this\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, net: NeuralNet, logic_fn: Callable[[Any, Any], Any]):\n",
    "        super(BaseLogicBenchmark, self).__init__(net)\n",
    "        self.maximize = False\n",
    "        self.targets = np.asarray(logic_fn(INPUTS[:, 0], INPUTS[:, 1]), dtype=float)[\n",
    "            :, None\n",
    "        ]\n",
    "\n",
    "    def evaluate_single(self, candidate: list[float]) -> int:\n",
    "        self.net.set_params(candidate)\n",
    "        self.net.reset(4)\n",
    "        # step once if no hidden\n",
    "        outputs = self.net.step(INPUTS)\n",
    "        # if hidden, step a second time so inputs reach outputs\n",
    "        if self.net.num_hidden > 0:\n",
    "            outputs = self.net.step(INPUTS)\n",
    "\n",
    "        # feed the inputs to the net and calculate error\n",
    "        return np.sum((outputs - self.targets) ** 2).item()\n",
    "\n",
    "\n",
    "class Or(BaseLogicBenchmark):\n",
    "    \"\"\"Defines Or Benchmark function, using a neural net\"\"\"\n",
    "\n",
    "    def __init__(self, num_hidden: int = 0, recurrent: bool = False):\n",
    "        super(Or, self).__init__(NeuralNet(2, 1, num_hidden, recurrent), np.logical_or)\n",
    "\n",
    "\n",
    "class And(BaseLogicBenchmark):\n",
    "    \"\"\"Defines OR Benchmark function, using a neural net\"\"\"\n",
    "\n",
    "    def __init__(self, num_hidden: int = 0, recurrent: bool = False):\n",
    "        super(And, self).__init__(\n",
    "            NeuralNet(2, 1, num_hidden, recurrent), np.logical_and\n",
    "        )\n",
    "\n",
    "\n",
    "class Xor(BaseLogicBenchmark):\n",
    "    \"\"\"Defines Xor Benchmark function, using a neural net\"\"\"\n",
    "\n",
    "    def __init__(self, num_hidden: int = 0, recurrent: bool = False):\n",
    "        super(Xor, self).__init__(\n",
    "            NeuralNet(2, 1, num_hidden, recurrent), np.logical_xor\n",
    "        )\n",
    "\n",
    "\n",
    "class TemporalLogicBenchmark(NeuralNetworkBenchmark):\n",
    "    \"\"\"Defines the base class for temporal logic\n",
    "    Neural Network Benchmark Problems. Other temporal logic neural net\n",
    "    benchmarks should inherit from this\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, net: NeuralNet, logic_fn: Callable[[Any, Any], Any]):\n",
    "        super(TemporalLogicBenchmark, self).__init__(net)\n",
    "        self.maximize = False\n",
    "        self.targets = np.asarray(logic_fn(INPUTS[:, 0], INPUTS[:, 1]), dtype=float)[\n",
    "            :, None\n",
    "        ]\n",
    "\n",
    "    def evaluate_single(self, candidate: list[float]) -> int:\n",
    "        self.net.set_params(candidate)\n",
    "        self.net.reset(4)\n",
    "\n",
    "        # for temporal xor we care about the output after seeing both inputs\n",
    "        outputs: NDArray[np.float64] = np.zeros((4, 1), dtype=float)\n",
    "        for i in range(2):\n",
    "            # step once if no hidden\n",
    "            outputs = self.net.step(INPUTS[:, i : i + 1])\n",
    "            # if hidden, step a second time for each input so inputs\n",
    "            # reach outputs\n",
    "            if self.net.num_hidden > 0:\n",
    "                outputs = self.net.step(INPUTS[:, i : i + 1])\n",
    "\n",
    "        # feed the inputs to the net and calculate error\n",
    "        return np.sum((outputs - self.targets) ** 2).item()\n",
    "\n",
    "\n",
    "class TemporalOr(TemporalLogicBenchmark):\n",
    "    \"\"\"Defines Temporal Or Benchmark function, using a neural net\"\"\"\n",
    "\n",
    "    def __init__(self, num_hidden: int = 0, recurrent: bool = False):\n",
    "        super(TemporalOr, self).__init__(\n",
    "            NeuralNet(1, 1, num_hidden, recurrent), np.logical_or\n",
    "        )\n",
    "\n",
    "\n",
    "class TemporalAnd(TemporalLogicBenchmark):\n",
    "    \"\"\"Defines OR Benchmark function, using a neural net\"\"\"\n",
    "\n",
    "    def __init__(self, num_hidden: int = 0, recurrent: bool = False):\n",
    "        super(TemporalAnd, self).__init__(\n",
    "            NeuralNet(1, 1, num_hidden, recurrent), np.logical_and\n",
    "        )\n",
    "\n",
    "\n",
    "class TemporalXor(TemporalLogicBenchmark):\n",
    "    \"\"\"Defines Temporal Xor Benchmark function, using a neural net\"\"\"\n",
    "\n",
    "    def __init__(self, num_hidden: int = 0, recurrent: bool = False):\n",
    "        super(TemporalXor, self).__init__(\n",
    "            NeuralNet(1, 1, num_hidden, recurrent), np.logical_xor\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.axes import Axes\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "from math import cos, sin, atan\n",
    "\n",
    "\n",
    "class ANNPlotter(object):\n",
    "    \"\"\"\n",
    "    Plot a neural network, positions determination based on code from:\n",
    "    https://stackoverflow.com/questions/29888233/how-to-visualize-a-neural-network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        net: NeuralNet,\n",
    "        axes: Optional[Axes] = None,\n",
    "        vertical_distance_between_layers: int = 10,\n",
    "        horizontal_distance_between_neurons: int = 10,\n",
    "        neuron_radius: float = 0.9,\n",
    "    ):\n",
    "        self.net = net\n",
    "        self.axes = plt.figure(\"ANN Plotter\").gca() if axes is None else axes\n",
    "\n",
    "        self.vertical_distance_between_layers = vertical_distance_between_layers\n",
    "        self.horizontal_distance_between_neurons = horizontal_distance_between_neurons\n",
    "        self.neuron_radius: float = neuron_radius\n",
    "\n",
    "        self.number_of_neurons_in_widest_layer: int = np.max([\n",
    "            net.num_inputs,\n",
    "            net.num_hidden,\n",
    "            net.num_outputs,\n",
    "        ])\n",
    "\n",
    "        # now calculate positions of all neurons\n",
    "        self.neuron_positions: list[list[tuple[float, float]]] = [[]]\n",
    "        x = self.__calculate_left_margin_so_layer_is_centered(self.net.num_inputs)\n",
    "        y = 0.0\n",
    "        for _ in range(self.net.num_inputs):\n",
    "            self.neuron_positions[-1].append((x, y))\n",
    "            x += self.horizontal_distance_between_neurons\n",
    "\n",
    "        if self.net.num_hidden > 0:\n",
    "            self.neuron_positions.append([])\n",
    "            y += self.vertical_distance_between_layers\n",
    "            x = self.__calculate_left_margin_so_layer_is_centered(self.net.num_hidden)\n",
    "            for _ in range(self.net.num_hidden):\n",
    "                self.neuron_positions[-1].append((x, y))\n",
    "                x += horizontal_distance_between_neurons\n",
    "\n",
    "        self.neuron_positions.append([])\n",
    "        y += self.vertical_distance_between_layers\n",
    "        x = self.__calculate_left_margin_so_layer_is_centered(self.net.num_outputs)\n",
    "        for _ in range(self.net.num_outputs):\n",
    "            self.neuron_positions[-1].append((x, y))\n",
    "            x += horizontal_distance_between_neurons\n",
    "\n",
    "        norm = colors.Normalize(vmin=-MAX_WEIGHT, vmax=MAX_WEIGHT)\n",
    "        self.scalar_map = plt.cm.ScalarMappable(norm=norm, cmap=\"jet\")\n",
    "        self.weights = []\n",
    "\n",
    "    def __calculate_left_margin_so_layer_is_centered(\n",
    "        self, number_of_neurons: int\n",
    "    ) -> float:\n",
    "        return (\n",
    "            self.horizontal_distance_between_neurons\n",
    "            * (self.number_of_neurons_in_widest_layer - number_of_neurons)\n",
    "        ) / 2.0\n",
    "\n",
    "    def __draw_synapse(\n",
    "        self,\n",
    "        neuron1: tuple[float, float],\n",
    "        neuron2: tuple[float, float],\n",
    "        weight: NDArray[np.float64],\n",
    "        recurrent: bool = False,\n",
    "    ):\n",
    "        color = self.scalar_map.to_rgba(weight)\n",
    "        self.weights.append(weight)\n",
    "\n",
    "        if not recurrent:\n",
    "            angle = atan((neuron2[0] - neuron1[0]) / float(neuron2[1] - neuron1[1]))\n",
    "\n",
    "            x_adjustment = self.neuron_radius * sin(angle)\n",
    "            y_adjustment = self.neuron_radius * cos(angle)\n",
    "\n",
    "            self.axes.arrow(\n",
    "                neuron1[0],\n",
    "                neuron1[1],\n",
    "                neuron2[0] - x_adjustment - neuron1[0],\n",
    "                neuron2[1] - y_adjustment - neuron1[1],\n",
    "                head_width=0.5,\n",
    "                head_length=0.5,\n",
    "                fc=color,\n",
    "                ec=color,\n",
    "                length_includes_head=True,\n",
    "                zorder=1,\n",
    "            )\n",
    "\n",
    "            # show weight 1/3 of way up connection line\n",
    "\n",
    "            label_x = neuron1[0] + (neuron2[0] - neuron1[0]) / 3.0\n",
    "            label_y = neuron1[1] + (neuron2[1] - neuron1[1]) / 3.0\n",
    "\n",
    "        else:\n",
    "            if neuron2[1] == neuron1[1]:  # same layer\n",
    "                height_factor = 1.5\n",
    "                width_factor = 2.0\n",
    "                if neuron1[0] > neuron2[0]:\n",
    "                    height_factor = 4.0\n",
    "                    width_factor = 4.0\n",
    "                verts = [\n",
    "                    neuron1,\n",
    "                    (\n",
    "                        neuron1[0]\n",
    "                        - self.horizontal_distance_between_neurons / width_factor,\n",
    "                        neuron1[1]\n",
    "                        + self.vertical_distance_between_layers / height_factor,\n",
    "                    ),\n",
    "                    (\n",
    "                        neuron2[0]\n",
    "                        + self.horizontal_distance_between_neurons / width_factor,\n",
    "                        neuron2[1]\n",
    "                        + self.vertical_distance_between_layers / height_factor,\n",
    "                    ),\n",
    "                    neuron2,\n",
    "                ]\n",
    "                codes = [Path.MOVETO, Path.CURVE4, Path.CURVE4, Path.CURVE4]\n",
    "            else:  # going to prior layer (actually doesn't exist right now)\n",
    "                factor = -2.0\n",
    "                if neuron2[0] > neuron1[0]:  # left to right\n",
    "                    factor = 2.0\n",
    "                verts = [\n",
    "                    neuron1,\n",
    "                    (\n",
    "                        neuron1[0] + self.horizontal_distance_between_neurons * factor,\n",
    "                        (neuron1[1] + neuron2[1]) / 2.0,\n",
    "                    ),\n",
    "                    neuron2,\n",
    "                ]\n",
    "                codes = [Path.MOVETO, Path.CURVE4, Path.CURVE4]\n",
    "\n",
    "            path = Path(verts, codes)\n",
    "\n",
    "            patch = patches.PathPatch(path, ec=color, fc=\"none\", zorder=1, aa=True)\n",
    "            # patch = patches.FancyArrowPatch(path=path, ec=color, fc=color, zorder=1, aa=True, arrowstyle=\"-|>\", mutation_scale=150**.5)\n",
    "            patch_verts: NDArray[np.float64] = patch.get_path().vertices  # type: ignore\n",
    "            self.axes.add_patch(patch)\n",
    "\n",
    "            if neuron1 == neuron2:\n",
    "                vert = patch_verts[int(len(patch_verts) / 2)]\n",
    "            else:\n",
    "                vert = patch_verts[int(len(patch_verts) / 3)]\n",
    "\n",
    "            label_x = vert[0]\n",
    "            label_y = vert[1]\n",
    "\n",
    "            if patch_verts[-2][1] != neuron2[1]:\n",
    "                angle = atan(\n",
    "                    (patch_verts[-2][0] - neuron2[0])\n",
    "                    / float(patch_verts[-2][1] - neuron2[1])\n",
    "                )\n",
    "                dx = 0.5 * sin(angle)\n",
    "                dy = 0.5 * cos(angle)\n",
    "                x_adjustment = self.neuron_radius * sin(angle)\n",
    "                y_adjustment = self.neuron_radius * cos(angle)\n",
    "\n",
    "                self.axes.arrow(\n",
    "                    neuron2[0] + x_adjustment + dx,\n",
    "                    neuron2[1] + y_adjustment + dy,\n",
    "                    -dx,\n",
    "                    -dy,\n",
    "                    head_width=0.5,\n",
    "                    head_length=0.5,\n",
    "                    fc=color,\n",
    "                    ec=color,\n",
    "                    length_includes_head=True,\n",
    "                    zorder=1,\n",
    "                    aa=True,\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                if neuron1[0] > neuron2[0]:\n",
    "                    dx = 0.5\n",
    "                else:\n",
    "                    dx = -0.5\n",
    "\n",
    "                self.axes.arrow(\n",
    "                    (neuron1[0] + neuron2[0]) / 2 - 0.1,\n",
    "                    patch_verts[int(len(patch_verts) / 2 - 1)][1] + 0.05,\n",
    "                    dx,\n",
    "                    0,\n",
    "                    head_width=0.5,\n",
    "                    head_length=0.5,\n",
    "                    fc=color,\n",
    "                    ec=color,\n",
    "                    length_includes_head=True,\n",
    "                    zorder=1,\n",
    "                    aa=True,\n",
    "                )\n",
    "\n",
    "        # use default box of white with white border\n",
    "        bbox = dict(\n",
    "            boxstyle=\"round\",\n",
    "            ec=(1.0, 1.0, 1.0),\n",
    "            fc=(1.0, 1.0, 1.0),\n",
    "        )\n",
    "        self.axes.text(\n",
    "            label_x,\n",
    "            label_y,\n",
    "            \"{:.2f}\".format(weight),\n",
    "            color=\"black\",\n",
    "            size=\"x-small\",\n",
    "            verticalalignment=\"center\",\n",
    "            horizontalalignment=\"center\",\n",
    "            bbox=bbox,\n",
    "            zorder=2,\n",
    "        )\n",
    "\n",
    "    def __draw_bias(self, layer: int) -> tuple[float, float]:\n",
    "        position = (\n",
    "            self.number_of_neurons_in_widest_layer\n",
    "            * self.horizontal_distance_between_neurons,\n",
    "            self.neuron_positions[layer][0][1]\n",
    "            + self.vertical_distance_between_layers / 2.0,\n",
    "        )\n",
    "        text = self.axes.text(\n",
    "            position[0],\n",
    "            position[1],\n",
    "            \"Bias\",\n",
    "            color=\"black\",\n",
    "            horizontalalignment=\"center\",\n",
    "            size=\"large\",\n",
    "        )\n",
    "        text.set_bbox(dict(edgecolor=\"black\", facecolor=\"lightgray\"))\n",
    "        text.set_zorder(2)\n",
    "        return position\n",
    "\n",
    "    def draw(self):\n",
    "        for layer in self.neuron_positions:\n",
    "            for neuron_position in layer:\n",
    "                circle = patches.Circle(\n",
    "                    neuron_position,\n",
    "                    radius=self.neuron_radius,\n",
    "                    ec=\"black\",\n",
    "                    fc=\"lightgray\",\n",
    "                    aa=True,\n",
    "                )\n",
    "                circle.set_zorder(2)\n",
    "                self.axes.add_patch(circle)\n",
    "\n",
    "        bias_positions = []\n",
    "        bias_positions.append(self.__draw_bias(0))\n",
    "        if self.net.num_hidden > 0:\n",
    "            bias_positions.append(self.__draw_bias(1))\n",
    "\n",
    "        if self.net.num_hidden == 0:\n",
    "            for j in range(self.net.num_outputs):\n",
    "                for i in range(self.net.num_inputs):\n",
    "                    self.__draw_synapse(\n",
    "                        self.neuron_positions[0][i],\n",
    "                        self.neuron_positions[1][j],\n",
    "                        self.net.weights[0][i][j],\n",
    "                    )\n",
    "                self.__draw_synapse(\n",
    "                    bias_positions[0],\n",
    "                    self.neuron_positions[1][j],\n",
    "                    self.net.weights[0][-1][j],\n",
    "                )\n",
    "            if self.net.recurrent:\n",
    "                # for no hidden units, but recurrent, we feedback outputs to self\n",
    "                for j in range(self.net.num_outputs):\n",
    "                    for i in range(self.net.num_outputs):\n",
    "                        self.__draw_synapse(\n",
    "                            self.neuron_positions[1][i],\n",
    "                            self.neuron_positions[1][j],\n",
    "                            self.net.weights[0][self.net.num_inputs + i][j],\n",
    "                            True,\n",
    "                        )\n",
    "        else:\n",
    "            for j in range(self.net.num_hidden):\n",
    "                for i in range(self.net.num_inputs):\n",
    "                    self.__draw_synapse(\n",
    "                        self.neuron_positions[0][i],\n",
    "                        self.neuron_positions[1][j],\n",
    "                        self.net.weights[0][i][j],\n",
    "                    )\n",
    "                self.__draw_synapse(\n",
    "                    bias_positions[0],\n",
    "                    self.neuron_positions[1][j],\n",
    "                    self.net.weights[0][-1][j],\n",
    "                )\n",
    "            for j in range(self.net.num_outputs):\n",
    "                for i in range(self.net.num_hidden):\n",
    "                    self.__draw_synapse(\n",
    "                        self.neuron_positions[1][i],\n",
    "                        self.neuron_positions[2][j],\n",
    "                        self.net.weights[1][i][j],\n",
    "                    )\n",
    "                self.__draw_synapse(\n",
    "                    bias_positions[1],\n",
    "                    self.neuron_positions[2][j],\n",
    "                    self.net.weights[1][-1][j],\n",
    "                )\n",
    "\n",
    "            if self.net.recurrent:\n",
    "                # for hidden + recurrent, we feedback hidden to hidden\n",
    "                for j in range(self.net.num_hidden):\n",
    "                    for i in range(self.net.num_hidden):\n",
    "                        self.__draw_synapse(\n",
    "                            self.neuron_positions[1][i],\n",
    "                            self.neuron_positions[1][j],\n",
    "                            self.net.weights[0][self.net.num_inputs + i][j],\n",
    "                            True,\n",
    "                        )\n",
    "\n",
    "        self.axes.relim()\n",
    "        self.axes.autoscale(tight=False)\n",
    "        xlim = self.axes.get_xlim()\n",
    "        self.axes.set_xlim(\n",
    "            xlim[0] - self.neuron_radius,\n",
    "            xlim[1] + self.horizontal_distance_between_neurons + self.neuron_radius,\n",
    "        )\n",
    "        self.axes.set_aspect(\"equal\")\n",
    "        # turn off all x axis ticks\n",
    "        self.axes.get_xaxis().set_ticks([])\n",
    "        self.axes.get_yaxis().set_ticks([\n",
    "            layer[0][1] for layer in self.neuron_positions\n",
    "        ])\n",
    "        if self.net.num_hidden > 0:\n",
    "            self.axes.get_yaxis().set_ticklabels([\"Input\", \"Hidden\", \"Output\"])\n",
    "        else:\n",
    "            self.axes.get_yaxis().set_ticklabels([\"Input\", \"Output\"])\n",
    "\n",
    "        self.scalar_map.set_array(np.asarray(self.weights))\n",
    "        plt.colorbar(self.scalar_map, ax=self.axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_utils import plot_results_1D, plot_results_2D, plot_observer\n",
    "from inspyred import benchmarks\n",
    "from inspyred.ec import (\n",
    "    terminators,\n",
    "    replacers,\n",
    "    selectors,\n",
    "    variators,\n",
    "    EvolutionaryComputation,\n",
    "    Individual,\n",
    ")\n",
    "import numpy as np\n",
    "from typing import Any\n",
    "from numpy.typing import NDArray\n",
    "from random import Random\n",
    "from utils.inspyred_utils import NumpyRandomWrapper\n",
    "\n",
    "\n",
    "def generate_offspring(\n",
    "    random: Random,\n",
    "    x0: list[float] | NDArray[np.float64],  # type: ignore\n",
    "    std_dev: float,\n",
    "    num_offspring: int,\n",
    "    display: bool,\n",
    "    kwargs: Any,\n",
    ") -> tuple[NDArray[np.float64], NDArray[np.float64]]:\n",
    "    x0: NDArray[np.float64] = np.asarray(x0, dtype=np.float64)\n",
    "\n",
    "    problem = benchmarks.Sphere(len(x0))\n",
    "\n",
    "    parent_fitness: NDArray[np.float64] = problem.evaluator([x0], None)[0]\n",
    "\n",
    "    algorithm = EvolutionaryComputation(random)\n",
    "    algorithm.terminator = terminators.generation_termination\n",
    "    algorithm.replacer = replacers.generational_replacement\n",
    "    algorithm.variator = variators.gaussian_mutation  # type: ignore\n",
    "\n",
    "    final_pop = algorithm.evolve(\n",
    "        generator=(lambda random, args: x0.copy()),  # type: ignore\n",
    "        evaluator=problem.evaluator,\n",
    "        pop_size=num_offspring,\n",
    "        maximize=False,\n",
    "        max_generations=1,\n",
    "        mutation_rate=1.0,\n",
    "        gaussian_stdev=std_dev,\n",
    "    )\n",
    "\n",
    "    offspring_fitnesses = np.asarray([guy.fitness for guy in final_pop])\n",
    "    offspring = np.asarray([guy.candidate for guy in final_pop])\n",
    "\n",
    "    if display:\n",
    "        # Plot the parent and the offspring on the fitness landscape\n",
    "        # (only for 1D or 2D functions)\n",
    "        if len(x0) == 1:\n",
    "            plot_results_1D(\n",
    "                problem,\n",
    "                x0,\n",
    "                parent_fitness,\n",
    "                offspring,\n",
    "                offspring_fitnesses,\n",
    "                \"Parent\",\n",
    "                \"Offspring\",\n",
    "                kwargs,\n",
    "            )\n",
    "\n",
    "        elif len(x0) == 2:\n",
    "            plot_results_2D(\n",
    "                problem, np.asarray([x0]), offspring, \"Parent\", \"Offspring\", kwargs\n",
    "            )\n",
    "\n",
    "    return (parent_fitness, offspring_fitnesses)\n",
    "\n",
    "\n",
    "def generator(random: Random, args: dict[str, Any]) -> NDArray[np.float64]:\n",
    "    return np.asarray([\n",
    "        random.uniform(args[\"pop_init_range\"][0], args[\"pop_init_range\"][1])\n",
    "        for _ in range(args[\"num_vars\"])\n",
    "    ])\n",
    "\n",
    "\n",
    "def initial_pop_observer(\n",
    "    population: list[Individual],\n",
    "    num_generations: int,\n",
    "    num_evaluations: int,\n",
    "    args: dict[str, Any],\n",
    ") -> None:\n",
    "    if num_generations == 0:\n",
    "        args[\"initial_pop_storage\"][\"individuals\"] = np.asarray([\n",
    "            guy.candidate for guy in population\n",
    "        ])\n",
    "        args[\"initial_pop_storage\"][\"fitnesses\"] = np.asarray([\n",
    "            guy.fitness for guy in population\n",
    "        ])\n",
    "\n",
    "\n",
    "def run_ga(\n",
    "    random: Random | NumpyRandomWrapper,\n",
    "    display: bool = False,\n",
    "    num_vars: int = 0,\n",
    "    problem_class: Any = benchmarks.Sphere,\n",
    "    maximize: bool = False,\n",
    "    use_bounder: bool = True,\n",
    "    **kwargs: Any,\n",
    ") -> tuple[NDArray[np.float64], float, list[Individual]]:\n",
    "    # create dictionaries to store data about initial population, and lines\n",
    "    initial_pop_storage = {}\n",
    "\n",
    "    algorithm = EvolutionaryComputation(random)\n",
    "    algorithm.terminator = terminators.generation_termination\n",
    "    algorithm.replacer = replacers.generational_replacement\n",
    "    algorithm.variator = [  # type: ignore\n",
    "        variators.uniform_crossover,\n",
    "        variators.gaussian_mutation,\n",
    "    ]\n",
    "    algorithm.selector = selectors.tournament_selection\n",
    "\n",
    "    if display:\n",
    "        algorithm.observer = [plot_observer, initial_pop_observer]  # type: ignore\n",
    "    else:\n",
    "        algorithm.observer = initial_pop_observer\n",
    "\n",
    "    kwargs[\"num_selected\"] = kwargs[\"pop_size\"]\n",
    "\n",
    "    if issubclass(problem_class.__class__, NeuralNetworkBenchmark):\n",
    "        if \"num_hidden_units\" not in kwargs:\n",
    "            kwargs[\"num_hidden_units\"] = 0\n",
    "        if \"recurrent\" not in kwargs:\n",
    "            kwargs[\"recurrent\"] = False\n",
    "        problem = problem_class(kwargs[\"num_hidden_units\"], kwargs[\"recurrent\"])  # type: ignore\n",
    "        num_vars = problem.dimensions\n",
    "        print(\"neural network benchmark\")\n",
    "    else:\n",
    "        print(\"not neural network benchmark\")\n",
    "        problem = problem_class(num_vars)\n",
    "\n",
    "    if use_bounder:\n",
    "        kwargs[\"bounder\"] = problem.bounder\n",
    "    if \"pop_init_range\" in kwargs:\n",
    "        kwargs[\"generator\"] = generator\n",
    "    else:\n",
    "        kwargs[\"generator\"] = problem.generator\n",
    "\n",
    "    final_pop: list[Individual] = algorithm.evolve(\n",
    "        evaluator=problem.evaluator,\n",
    "        maximize=problem.maximize,\n",
    "        initial_pop_storage=initial_pop_storage,\n",
    "        num_vars=num_vars,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    # best_guy = final_pop[0].candidate\n",
    "    # best_fitness = final_pop[0].fitness\n",
    "    final_pop_fitnesses: NDArray[np.float64] = np.asarray([\n",
    "        guy.fitness for guy in final_pop\n",
    "    ])\n",
    "    final_pop_candidates: NDArray[np.float64] = np.asarray([\n",
    "        guy.candidate for guy in final_pop\n",
    "    ])\n",
    "\n",
    "    sort_indexes = sorted(\n",
    "        range(len(final_pop_fitnesses)), key=final_pop_fitnesses.__getitem__\n",
    "    )\n",
    "    final_pop_fitnesses = final_pop_fitnesses[sort_indexes]\n",
    "    final_pop_candidates = final_pop_candidates[sort_indexes]\n",
    "\n",
    "    best_guy: NDArray[np.float64] = final_pop_candidates[0]\n",
    "    best_fitness: float = final_pop_fitnesses[0]\n",
    "\n",
    "    if display:\n",
    "        # Plot the parent and the offspring on the fitness landscape\n",
    "        # (only for 1D or 2D functions)\n",
    "        if num_vars == 1:\n",
    "            plot_results_1D(\n",
    "                problem,\n",
    "                initial_pop_storage[\"individuals\"],\n",
    "                initial_pop_storage[\"fitnesses\"],\n",
    "                final_pop_candidates,\n",
    "                final_pop_fitnesses,\n",
    "                \"Initial Population\",\n",
    "                \"Final Population\",\n",
    "                kwargs,\n",
    "            )\n",
    "\n",
    "        elif num_vars == 2:\n",
    "            plot_results_2D(\n",
    "                problem,\n",
    "                initial_pop_storage[\"individuals\"],\n",
    "                final_pop_candidates,\n",
    "                \"Initial Population\",\n",
    "                \"Final Population\",\n",
    "                kwargs,\n",
    "            )\n",
    "\n",
    "    return best_guy, best_fitness, final_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "In the first two exercises you will investigate running an EA to evolve the weights of an Artificial Neural Network (ANN). While there are other ways to learn the weights of an ANN (gradient based methods, such as backpropagation and alike), using evolution is an effective means in many circumstances. In this first exercise we will evolve the weights of a simple Feed Forward Neural Network (FFNN), while in the next one we will evolve the weights of more complex, recurrent neural nets. In both cases, we will assume a fixed topology with one input layer with as many nodes as the inputs, one output layer with a single node, and one (optional) hidden layer with a predefined number of nodes. Weights range in $[-8,8]$ (with real-valued encoding). All nodes of these Neural Networks use the logistic activation function (sigmoid):\n",
    "$f(x) = \\frac{1}{1+e^{-x}}$.\n",
    "\n",
    "We begin by evolving the weights of a minimal Neural Network to solve the ``Or`` problem. That means we will use a Neural Network that has two inputs, and one output, which should produce the logical ``Or`` function of the two input values, see the truth table shown in table below.\n",
    "\n",
    "| **Input 1** | **Input 2** | **Output** |\n",
    "|:-----------:|:-----------:|:----------:|\n",
    "| 0           | 0           | 0          |\n",
    "| 0           | 1           | 1          |\n",
    "| 1           | 0           | 1          |\n",
    "| 1           | 1           | 1          |\n",
    "\n",
    "\n",
    "Run the code below to evolve the weights for this ``Or`` network. The fitness here is the sum of squared errors between the network's output and the target output across each of the four input patterns. If you see the best fitness approach zero (e.g. go under 0.1) then you have found a network able to solve this problem. This most likely looks similar to what shown in the Figure below. \n",
    "\n",
    "![A graphical representation of the evolved Neural Network for the ``Or`` problem\"](img/ann_or.png)\n",
    "\n",
    "Here the Neural Network is depicted with its weights and biases shown by the corresponding color. If you were able to solve the ``Or`` problem, look at the weights of the Neural Network (note that, in the terminal, weights appear ordered by layer and, for each layer, by node, the bias weight being the last one) and think about / compute how it behaves when given different input patterns. Try to plug manually different couples of Input 1 and Input 2 into the network, and calculate the corresponding Output. It is important to think about this now, because it will be difficult to keep track of what our Neural Networks are doing once we start using more complex topologies.\n",
    "\n",
    "If you were not able to solve the ``Or`` problem, try modifying some of the EA parameters (see the comments in the script), until you are able to do so.\n",
    "\n",
    "Once you are able to solve ``Or``, try solving the ``And`` problem instead (change in the script ``problem_class=Or`` to ``problem_class=And``).\n",
    "\n",
    "- Do the same EA parameters that you used for ``Or`` work for ``And`` as well? If not, modify them until you are able to solve ``And``.\n",
    "\n",
    "\n",
    "Now that we can solve ``Or`` and ``And``, we will try something a little more challenging. Change the parameter ``problem_class`` to be ``Xor``, so that we are now trying to solve the ``Exclusive Or`` (``Xor``) function, see the truth table shown in the Table below.\n",
    "\n",
    "| **Input 1** | **Input 2** | **Output**           |\n",
    "|:-----------:|:-----------:|:--------------------:|\n",
    "| 0           | 0           | 0                    |\n",
    "| 0           | 1           | 1                    |\n",
    "| 1           | 0           | 1                    |\n",
    "| 1           | 1           |<span style=\"color:red\">0</span>|\n",
    "\n",
    "\n",
    "This function has one small, but crucial difference from ``Or`` (highlighted in red), as can be seen by comparing their truth tables.\n",
    "\n",
    "Try running the code again after changing ``problem_class`` to ``Xor``.\n",
    "- Can you solve it? If you are unable to solve it, why is that?\n",
    "\n",
    "In this case, it is worth considering an additional parameter that can be tuned, that is the number of hidden nodes of the Neural Network (parameter ``args[\"num_hidden_units\"]``. Try changing this parameter from 0 to 1 (this will add to the topology a hidden layer with one node).\n",
    "\n",
    "- Does this allow you to solve the problem? What if you change this value to 2 or more?\n",
    "- How many hidden nodes are required to solve this problem? Can you provide an explanation for why that is the case?\n",
    "\n",
    "When you find a network that does compute ``Xor``, once again see if you can understand how the network does so. Try to plug manually different couples of Input 1 and Input 2 into the network, and calculate the corresponding Output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import Random\n",
    "\n",
    "\n",
    "args = {}\n",
    "\n",
    "\"\"\"   \n",
    "-------------------------------------------------------------------------\n",
    "Edit this part to do the exercises\n",
    "\"\"\"\n",
    "\n",
    "# problem\n",
    "problem_class = Xor  # And, Xor\n",
    "\n",
    "# parameters for the GA\n",
    "args[\"num_hidden_units\"] = 2  # Number of hidden units of the neural network\n",
    "args[\"gaussian_stdev\"] = 1.0  # Standard deviation of the Gaussian mutations\n",
    "args[\"crossover_rate\"] = 0.8  # Crossover fraction\n",
    "args[\"tournament_size\"] = 2  # Tournament size\n",
    "args[\"pop_size\"] = 10  # Population size\n",
    "\n",
    "args[\"num_elites\"] = 1  # number of elite individuals to maintain in each gen\n",
    "args[\"mutation_rate\"] = 0.8  # fraction of loci to perform mutation on\n",
    "\n",
    "# by default will use the problem's defined init_range\n",
    "# uncomment the following line to use a specific range instead\n",
    "# args[\"pop_init_range\"] = [-500, 500] # Range for the initial population\n",
    "\n",
    "args[\"use_bounder\"] = True  # use the problem's bounder to restrict values\n",
    "# comment out the previously line to run unbounded\n",
    "\n",
    "args[\"max_generations\"] = 100  # Number of generations of the GA\n",
    "display = True  # Plot initial and final populations\n",
    "\n",
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "args[\"fig_title\"] = \"GA\"\n",
    "\n",
    "\n",
    "rng = Random(x=41)\n",
    "\n",
    "best_individual, best_fitness, final_pop = run_ga(\n",
    "    rng, display=display, problem_class=problem_class, **args\n",
    ")\n",
    "print(\"Best Individual\", best_individual)\n",
    "print(\"Best Fitness\", best_fitness)\n",
    "\n",
    "if display:\n",
    "    net = problem_class(args[\"num_hidden_units\"]).net\n",
    "    net.set_params(best_individual)\n",
    "\n",
    "    ann_plotter = ANNPlotter(net)\n",
    "    ann_plotter.draw()\n",
    "\n",
    "    plt.ioff()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "So far we have used Neural Networks for solving tasks where the output depends *statically* on the input vector, i.e., the input-output dependency does not change over time. However, there are many tasks such as time-series forecast and some robotic applications where the input-output dependence is *dynamic*, i.e., the output of the system at time $t$ depends on the inputs at the same step, but also on the inputs at the previous time step(s).\n",
    "First, we start with a modified version of the ``Or`` problem, that is called ``Temporal Or``. While the basic ``Or`` problem involved evolving a Neural Network that would give a single output when provided two simultaneous inputs, in ``Temporal Or``, there is only a single input node and the input values are provided in sequence. Therefore the network will have to remember the first input when seeing the second, in order to output the correct value.\n",
    "\n",
    "Run the jupyter block code to solve ``Temporal Or``.\n",
    "- Can you solve it? If you are unable to solve it, why is that?\n",
    "\n",
    "In this case, notice that there is one new parameter that you can modify in the script: ``recurrent``. This parameter is a Boolean flag, that says whether the network is recurrent (in this case an Elman network, [link here](https://en.wikipedia.org/wiki/Recurrent\\_neural\\_network\\#Elman\\_networks\\_and\\_Jordan\\_networks)) or not (in which case it is a FFNN).\n",
    "- If you set ``recurrent`` to be ``True``, can you now evolve a successful network?\n",
    "- Why might recurrence be important for solving a temporal problem such as this?\n",
    "  \n",
    "Once you have been able to evolve a network capable of solving ``Temporal Or``, you can change in the script the parameter ``problem_class`` to ``TemporalAnd``, to attempt solving a temporal version of ``And``, and repeat.\n",
    "- Do the same EA parameters that solved ``Temporal Or`` also work for ``Temporal And``?\n",
    "- Why, or why not?\n",
    "\n",
    "Finally, change in the script the parameter ``problem_class`` to ``TemporalXor``, to attempt solving a temporal version of ``Xor``. Run the code again.\n",
    "- Are you able to find a successful network?\n",
    "- If not, think back to what you just saw in the previous exercise. What combination of recurrence and no. of hidden nodes is needed to solve ``Temporal Xor`` and why is that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "args = {}\n",
    "\n",
    "\"\"\"   \n",
    "-------------------------------------------------------------------------\n",
    "Edit this part to do the exercises\n",
    "\"\"\"\n",
    "\n",
    "# problem\n",
    "problem_class = TemporalXor\n",
    "\n",
    "# parameters for the GA\n",
    "args[\"num_hidden_units\"] = 2  # Number of hidden units of the neural network\n",
    "args[\"recurrent\"] = True  # Number of hidden units of the neural network\n",
    "args[\"gaussian_stdev\"] = 1.0  # Standard deviation of the Gaussian mutations\n",
    "args[\"crossover_rate\"] = 0.8  # Crossover fraction\n",
    "args[\"tournament_size\"] = 2  # Tournament size\n",
    "args[\"pop_size\"] = 100  # Population size\n",
    "\n",
    "args[\"num_elites\"] = 1  # number of elite individuals to maintain in each gen\n",
    "args[\"mutation_rate\"] = 0.5  # fraction of loci to perform mutation on\n",
    "\n",
    "# by default will use the problem's defined init_range\n",
    "# uncomment the following line to use a specific range instead\n",
    "# args[\"pop_init_range\"] = [-500, 500] # Range for the initial population\n",
    "\n",
    "args[\"use_bounder\"] = True  # use the problem's bounder to restrict values\n",
    "# comment out the previously line to run unbounded\n",
    "\n",
    "args[\"max_generations\"] = 100  # Number of generations of the GA\n",
    "display = True  # Plot initial and final populations\n",
    "\n",
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "args[\"fig_title\"] = \"GA\"\n",
    "\n",
    "rng = Random(x=41)\n",
    "\n",
    "best_individual, best_fitness, final_pop = run_ga(\n",
    "    rng, display=display, problem_class=problem_class, **args\n",
    ")\n",
    "print(\"Best Individual\", best_individual)\n",
    "print(\"Best Fitness\", best_fitness)\n",
    "\n",
    "if display:\n",
    "    net = problem_class(args[\"num_hidden_units\"], args[\"recurrent\"]).net\n",
    "    net.set_params(best_individual)\n",
    "\n",
    "    ann_plotter = ANNPlotter(net)\n",
    "    ann_plotter.draw()\n",
    "\n",
    "    plt.ioff()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "In this exercise we will use the Python implementation of Neural Evolution of Augmenting Topologies (NEAT) provided by *neat-python*, to solve the ``Xor`` problem we have seen in the first exercise. The main difference is that in this case we won't fix the network topology *a priori* and evolve its weights, but rather we will evolve the weights *and* the network topology itself. \n",
    "\n",
    "**NOTE**: In this case NEAT is configured to *maximize* the number of correct outputs in the $4$ input cases of the ``Xor`` truth table, therefore the optimal fitness value is $4$.\n",
    "\n",
    "See the jupyter block code and try to understand its main steps. The *neat-python* library allows to configure all the algorithmic details of NEAT by means of an external configuration file. In this exercise two different configuration files **(inside folder utils/utils_08)** will be used, namely:\n",
    "-  ``config-feedforward-2input-xor-noelitism.txt``\n",
    "-  ``config-feedforward-2input-xor-elitism.txt``\n",
    "\n",
    "Spend some time on one of the two files to get an idea about the main configurations that can be changed in NEAT ([see link](https://neat-python.readthedocs.io)). As you will see, in this case the two configuration files are pretty much the same, except for two parameters, namely ``species_elitism`` and ``elitism``. These represent respectively the no. of elite species (remember from the lecture that NEAT uses a **speciation** mechanism to allow mating only of *similar* networks) and elite individuals (i.e., networks) that are kept in the population. More specifically,``config-feedforward-2input-xor-noelitism.txt`` sets both parameters to zero, while ``config-feedforward-2input-xor-elitism.txt`` sets them to two, meaning that two elite species and two elite networks are kept.\n",
    "\n",
    "The script can be configured either to run a single instance of NEAT (by setting ``num_runs=1``) on one of the two configurations, or multiple runs (by setting ``num_runs`` to values bigger than one) on one or both configurations. In the first case, you can choose one of the two configuration files (with/without elitism) and observe how a single run of each configuration performs. The script will log on the console the runtime details of the evolutionary process (to disable this feature, simply comment the line ``p.add_reporter(neat.StdOutReporter(True)``). At the end of the run, you should obtain two figures similar to those shown below. In the second case, the script will execute multiple runs (e.g. $10$) of both configurations, and then produce a boxplot comparing the best fitness obtained by each configuration at the end of the computational budget. By default, the stop condition is set to $100$ generations, see the parameter ``num_generations`` in the code. However, the algorithm has an additional stop criterion, i.e., it stops when it obtains a Neural Network whose fitness is higher than the parameter ``fitness_threshold`` in the corresponding configuration file (in this case, by default this parameter is set to $3.9$, sufficiently close to the optimal value of $4$ to be approximated by a sigmoid function).\n",
    "\n",
    "Fitness trend for the ``Xor`` problem solved by NEAT |  Corresponding species evolution, where each stacked plot represents one species and its size over evolutionary time. Species can go extinct if their size goes to zero\n",
    "- | - \n",
    "![alt](img/trend.png) | ![alt](img/species.png)\n",
    "\n",
    "- First, run a single instance of each of the two configurations (with/without elitism, simply change ``config_files[0]`` to ``config_files[1]``). What do you observe? Is the algorithm without elitism able to converge to the optimal fitness value? What about the algorithm with elitism? What is the effect of elitism on convergence? What about the number of species and their dynamics?\n",
    "- Change the parameter ``num_runs`` to $10$ or more. Does the boxplot confirm -in statistical terms- what you observed on a single run? (**NOTE**: it takes 1-2 minutes to execute 10 runs for both configurations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any\n",
    "import neat\n",
    "from utils.plot_utils import draw_net, plot_stats, plot_species\n",
    "import matplotlib.pyplot as plt\n",
    "from neat import Config\n",
    "\n",
    "\"\"\"\n",
    "2-input XOR\n",
    "\"\"\"\n",
    "\n",
    "# 2-input XOR inputs and expected outputs.\n",
    "inputs = [(0.0, 0.0), (0.0, 1.0), (1.0, 0.0), (1.0, 1.0)]\n",
    "outputs = [(0.0,), (1.0,), (1.0,), (0.0,)]\n",
    "\n",
    "num_generations = 100\n",
    "num_runs = 1\n",
    "\n",
    "config_files = [\n",
    "    \"../../datasets/lab08/config-feedforward-2input-xor-noelitism.txt\",\n",
    "    \"../../datasets/lab08/config-feedforward-2input-xor-elitism.txt\",\n",
    "]\n",
    "\n",
    "\n",
    "def eval_genomes(genomes: Any, config: Config) -> None:\n",
    "    for _, genome in genomes:\n",
    "        genome.fitness = len(inputs)\n",
    "        net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
    "        for xi, xo in zip(inputs, outputs):\n",
    "            output = net.activate(xi)\n",
    "            genome.fitness -= (output[0] - xo[0]) ** 2\n",
    "\n",
    "\n",
    "local_dir = os.path.dirname(\"labs/lab08\")\n",
    "\n",
    "if num_runs == 1:\n",
    "    # Load configuration.\n",
    "    config_file = config_files[0]\n",
    "    config = neat.Config(\n",
    "        neat.DefaultGenome,\n",
    "        neat.DefaultReproduction,\n",
    "        neat.DefaultSpeciesSet,\n",
    "        neat.DefaultStagnation,\n",
    "        config_file,\n",
    "    )\n",
    "\n",
    "    # Create the population, which is the top-level object for a NEAT run.\n",
    "    p = neat.Population(config)\n",
    "\n",
    "    # Add a stdout reporter to show progress in the terminal.\n",
    "    stats = neat.StatisticsReporter()\n",
    "    p.add_reporter(neat.StdOutReporter(True))\n",
    "    p.add_reporter(stats)\n",
    "\n",
    "    # run NEAT for num_generations\n",
    "    winner = p.run(eval_genomes, num_generations)\n",
    "\n",
    "    # Display the winning genome.\n",
    "    print(\"\\nBest genome:\\n{!s}\".format(winner))\n",
    "\n",
    "    # Show output of the most fit genome against training data.\n",
    "    print(\"\\nOutput:\")\n",
    "    winner_net = neat.nn.FeedForwardNetwork.create(winner, config)\n",
    "    for xi, xo in zip(inputs, outputs):\n",
    "        output = winner_net.activate(xi)\n",
    "        print(\"input {!r}, expected output {!r}, got {!r}\".format(xi, xo, output))\n",
    "\n",
    "    node_names = {-1: \"A\", -2: \"B\", 0: \"A XOR B\"}\n",
    "    draw_net(config, winner, filename=\"2-input OR\", view=True, node_names=node_names)\n",
    "    plot_stats(stats, ylog=False, view=True)\n",
    "    plot_species(stats, view=True)\n",
    "\n",
    "else:\n",
    "    results = []\n",
    "    for file in config_files:\n",
    "        # Load configuration.\n",
    "        config_file = file\n",
    "        config = neat.Config(\n",
    "            neat.DefaultGenome,\n",
    "            neat.DefaultReproduction,\n",
    "            neat.DefaultSpeciesSet,\n",
    "            neat.DefaultStagnation,\n",
    "            config_file,\n",
    "        )\n",
    "\n",
    "        best_fitnesses: list[float] = []\n",
    "        for i in range(num_runs):\n",
    "            print(\"{0}/{1}\".format(i + 1, num_runs))\n",
    "            p = neat.Population(config)\n",
    "            winner = p.run(eval_genomes, num_generations)\n",
    "            best_fitnesses.append(winner.fitness)  # type: ignore\n",
    "        results.append(best_fitnesses)\n",
    "\n",
    "    fig = plt.figure(\"NEAT\")\n",
    "    ax = fig.gca()\n",
    "    ax.boxplot(results)\n",
    "    ax.set_xticklabels([\"Without elitism\", \"With elitism\"])\n",
    "    # ax.set_yscale('log')\n",
    "    ax.set_xlabel(\"Condition\")\n",
    "    ax.set_ylabel(\"Best fitness\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "In this exercise we use again NEAT, this time to solve a 3-input Boolean function described by the truth table shown in the Table below. This function returns $1$ if and only if only one input is equal to $1$, otherwise it returns $0$.\n",
    "\n",
    "| **Input 1** | **Input 2** | **Input 3** | **Output** |\n",
    "|:-----------:|:-----------:|:-----------:|:----------:|\n",
    "| 0           | 0           | 0           | 0          |\n",
    "| 0           | 0           | 1           | 1          |\n",
    "| 0           | 1           | 0           | 1          |\n",
    "| 0           | 1           | 1           | 0          |\n",
    "| 1           | 0           | 0           | 1          |\n",
    "| 1           | 0           | 1           | 0          |\n",
    "| 1           | 1           | 0           | 0          |\n",
    "| 1           | 1           | 1           | 0          |\n",
    "\n",
    "\n",
    "This script has the same structure that we have seen in the previous exercise. In this exercise two different configuration files will be used, namely:\n",
    "- ``config-feedforward-3input-function-nohidden.txt``\n",
    "- ``config-feedforward-3input-function-hidden.txt``\n",
    "\n",
    "In this case the only difference between the two configurations (which both use elitism) is the number of hidden nodes to add to each genome in the initial population (parameter ``num_hidden``), which is set respectively to $0$ and $3$ (also, note that the parameter ``num_inputs`` is set to $3$ to allow the use of $3$ inputs, while in the previous exercise it was set to $2$). Note that as in this case the optimal fitness value is $8$ (i.e., the Neural Network output is correct in all 8 input cases), in both configuration files the parameter ``fitness_threshold`` is set to $7.9$.\n",
    "\n",
    "- What do you observe in this case when you execute a single run of each configuration? What is the effect of using hidden nodes in the initial population?\n",
    "- What happens when you configure the script to execute multiple runs? Does the boxplot confirm -in statistical terms- what you observed on a single run? (**NOTE**: it takes 1-2 minutes to execute 10 runs for both configurations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neat\n",
    "from utils.plot_utils import draw_net, plot_stats, plot_species\n",
    "from neat import Config\n",
    "from typing import Any\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 3-input Boolean function inputs and expected outputs.\n",
    "inputs = [\n",
    "    (0.0, 0.0, 0.0),\n",
    "    (0.0, 0.0, 1.0),\n",
    "    (0.0, 1.0, 0.0),\n",
    "    (0.0, 1.0, 1.0),\n",
    "    (1.0, 0.0, 0.0),\n",
    "    (1.0, 0.0, 1.0),\n",
    "    (1.0, 1.0, 0.0),\n",
    "    (1.0, 1.0, 1.0),\n",
    "]\n",
    "outputs = [(0.0,), (1.0,), (1.0,), (0.0,), (1.0,), (0.0,), (0.0,), (0.0,)]\n",
    "\n",
    "num_generations = 100\n",
    "num_runs = 1\n",
    "\n",
    "config_files = [\n",
    "    \"../../datasets/lab08/config-feedforward-3input-function-nohidden.txt\",\n",
    "    \"../../datasets/lab08/config-feedforward-3input-function-hidden.txt\",\n",
    "]\n",
    "\n",
    "\n",
    "def eval_genomes(genomes: Any, config: Config) -> None:\n",
    "    for _, genome in genomes:\n",
    "        genome.fitness = len(inputs)\n",
    "        net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
    "        for xi, xo in zip(inputs, outputs):\n",
    "            output = net.activate(xi)\n",
    "            genome.fitness -= (output[0] - xo[0]) ** 2\n",
    "\n",
    "\n",
    "if num_runs == 1:\n",
    "    # Load configuration.\n",
    "    config_file = config_files[0]\n",
    "    config = neat.Config(\n",
    "        neat.DefaultGenome,\n",
    "        neat.DefaultReproduction,\n",
    "        neat.DefaultSpeciesSet,\n",
    "        neat.DefaultStagnation,\n",
    "        config_file,\n",
    "    )\n",
    "\n",
    "    # Create the population, which is the top-level object for a NEAT run.\n",
    "    p = neat.Population(config)\n",
    "\n",
    "    # Add a stdout reporter to show progress in the terminal.\n",
    "    stats = neat.StatisticsReporter()\n",
    "    p.add_reporter(neat.StdOutReporter(True))\n",
    "    p.add_reporter(stats)\n",
    "\n",
    "    # run NEAT for num_generations\n",
    "    winner = p.run(eval_genomes, num_generations)\n",
    "\n",
    "    # Display the winning genome.\n",
    "    print(\"\\nBest genome:\\n{!s}\".format(winner))\n",
    "\n",
    "    # Show output of the most fit genome against training data.\n",
    "    print(\"\\nOutput:\")\n",
    "    winner_net = neat.nn.FeedForwardNetwork.create(winner, config)\n",
    "    for xi, xo in zip(inputs, outputs):\n",
    "        output = winner_net.activate(xi)\n",
    "        print(\"input {!r}, expected output {!r}, got {!r}\".format(xi, xo, output))\n",
    "\n",
    "    node_names = {-1: \"A\", -2: \"B\", -3: \"C\", 0: \"f(A,B,C)\"}\n",
    "    draw_net(\n",
    "        config,\n",
    "        winner,\n",
    "        filename=\"3-input Bool function\",\n",
    "        view=True,\n",
    "        node_names=node_names,\n",
    "    )\n",
    "    plot_stats(stats, ylog=False, view=True)\n",
    "    plot_species(stats, view=True)\n",
    "\n",
    "else:\n",
    "    results = []\n",
    "    for file in config_files:\n",
    "        # Load configuration.\n",
    "        config_file = file\n",
    "        config = neat.Config(\n",
    "            neat.DefaultGenome,\n",
    "            neat.DefaultReproduction,\n",
    "            neat.DefaultSpeciesSet,\n",
    "            neat.DefaultStagnation,\n",
    "            config_file,\n",
    "        )\n",
    "\n",
    "        best_fitnesses = []\n",
    "        for i in range(num_runs):\n",
    "            print(\"{0}/{1}\".format(i + 1, num_runs))\n",
    "            p = neat.Population(config)\n",
    "            winner = p.run(eval_genomes, num_generations)\n",
    "            best_fitnesses.append(winner.fitness)  # type: ignore\n",
    "        results.append(best_fitnesses)\n",
    "\n",
    "    fig = plt.figure(\"NEAT\")\n",
    "    ax = fig.gca()\n",
    "    ax.boxplot(results)\n",
    "    ax.set_xticklabels([\"Without hidden nodes\", \"With hidden nodes\"])\n",
    "    # ax.set_yscale('log')\n",
    "    ax.set_xlabel(\"Condition\")\n",
    "    ax.set_ylabel(\"Best fitness\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Instruction and questions\n",
    "Concisely note down your observations from the previous exercises (follow the bullet points) and think about the following questions. \n",
    "- What is the genotype and what is the phenotype in the problems considered in this lab?\n",
    "- Why are hidden nodes sometimes needed for a Neural Network to solve a given task? What is the defining feature of problems that networks without hidden nodes are unable to solve?\n",
    "- Why are recurrent connections needed to solve certain problems? What is the defining feature of problems that networks without recurrent connections are unable to solve? Are there problems that require recurrent connections and multiple hidden nodes?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
